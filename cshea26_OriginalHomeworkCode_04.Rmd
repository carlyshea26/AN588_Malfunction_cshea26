---
title: "cshea26_OriginalHomeworkCode_04"
author: "Carly S McDermott"
date: "2025-03-14"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

INFO: 
- Z and T tests are used to evaluate whether a given sample statistic (e.g., a mean or proportion) deviates significantly from what is expected under a null model or whether two samples statistics deviate significantly from one another.

- We REJECT a H0 if the p value obtained for a given Z or T test statistic is < α
CIs for our sample statistic are calculated as mean ± T(1−α/2) or Z(1−α/2) x SEM, and we can REJECT a H0 if the (1-α) CI around does not include the expected value of the statistic
When the sample size > 30, or when we are dealing with proportions, we use Z quantiles for calculating CIs and p values, but for sample size < 30, we use T quantiles



## 1) Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
- Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().
- When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().
The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.
- The function should contain a check for the rules of thumb we have talked about (\(n * p > 5\) and \(n * (1-p) > 5\)) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.
- The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). - For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.

```{r}
Z.prop.test <- function(p1, n1, p2 = NULL, n2 = NULL, p0, alternative = "two.sided", conf.level = 0.95) {
  if (is.null(p2) || is.null(n2)) {
    z <- (p1 - p0)/sqrt(p0 * (1 - p0)/n1) #mean of sample observations minus expected mean, divided by sample standard of deviations divided by square root of number of sample observations
    if (alternative == "greater") {
      pval <- pnorm(z, lower.tail = FALSE) #checking if alternative is greater or less than (indicates if we need to use upper or lower tail)
    } else if (alternative == "less") {
      pval <- pnorm(z, lower.tail = TRUE)
    } else if (alternative == "two.sided") { #otherwise checking if z is greater than zero, if so we do a lower tail test, if not we do an upper tail test
      if (z > 0) {
        pval <- 2 * pnorm(z, lower.tail = FALSE)
      } else {
        pval <- 2 * pnorm(z, lower.tail = TRUE)
      }
    }
    
    #creating warning message to ensure the validity of assuming the normal distribution
    if ((n1 * p0 > 5) || (n1 * (1-p0) > 5)) {
      warning("invalid assumption of normal distribution") 
    }
    
    lower <- p1 - qnorm(0.975) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm(0.975) * sqrt(p1 * (1 - p1)/n1)
    ci <- c(lower, upper) #creating confidence interval based on upper and lower bounds (.975 and .025)
    
    result <- list(test.type = "One-Sample Proportion Z-test", 
                                alternative = alternative,
                                z.test.stat = as.numeric(z), 
                                p.value = as.numeric(pval), 
                                confidence.interval = ci)
    return(result)
  } else {
    pstar <- ((p1*n1) + (p2*n2))/(n1 + n2) 
    
    z <- (p2 - p1)/sqrt((pstar * (1 - pstar)) * (1/n1 + 1/n2))
    
    if (alternative == "greater") {
        pval <- pnorm(z, lower.tail = FALSE)
      }
      if (alternative == "less") {
        pval <- pnorm(z, lower.tail = TRUE)
      }
      if (alternative == "two.sided") {
        pval <- 1 - pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
          }
    
    if ((n1 * p0 < 5) | (n1 * (1 - p0) < 5 ) | (n2 * p0 < 5) | (n2 * (1 - p0) < 5)) {
      warning("invalid assumption of normal distribution")
  }
  a = 1 - (conf.level)
  crit <- qnorm(1 - a/2)
  
  upper <- (p1 - p2) + (crit) * (sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
  lower <- (p1 - p2) - (crit) * (sqrt((p1*(1-p1)/n1) + (p2 * (1-p2)/n2)))
  ci <- c(lower, upper)
  
    result2 <- list(test.type = "Two-Sample Proportion Z-test", 
                                alternative = alternative,
                                z.test.statistic = as.numeric(z), 
                                p.value = as.numeric(pval), 
                                confidence.interval = ci, 
                                critical.value = as.numeric(crit))
    return(result2)
  }
}


#need to create test now to validate results
```

## 2) The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. 
- For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

Part 1: Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text())
```{r}
library(curl)
library(ggplot2)
library(ggpubr) 

f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
monkey <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)


monkey_clean <- na.omit(monkey[, c("MaxLongevity_m", "Brain_Size_Species_Mean")]) # clean data to get longevity and brain size

longevity <- monkey_clean$MaxLongevity_m
brain <- monkey_clean$Brain_Size_Species_Mean #save brain size and longevity into variables

z <- data.frame(x = longevity, y = brain) # create a dataframe to display longevity and brain size means

model <- lm(y ~ x, data = z) # save lm func result in model

intercept <- round(coef(model)[1], 2) # save x and y vals 
slope <- round(coef(model)[2], 2)
equation <- paste("y =", slope, "* x +", intercept) # format equation for geom_text header

g <- ggplot(data = z, aes(x = x, y = y)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) +
  geom_text(x = min(z$x), y = max(z$y), label = equation, hjust = 0, size = 5) +
  labs(title = "Brain Size vs. Longevity", x = "Longevity", y = "Brain Size") +
  theme_minimal() # create a graph to display

g # shows the graph

```
STILL NEED TO DO - PLOTTING LOGS: 
```{r}

```



Part 2: Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

An equation to use to calculate beta 1: 
- beta1 <- cor(w, h) * (sd(h)/sd(w))
where 
- w <- d$weight (longevity)
- h <- d$height (brain size)
```{r}
#assigning variables similar to module 12
ci <- predict(model, newdata = data.frame(brain), interval = "confidence", level = 0.90) ##calculate the confidence interval

info <- data.frame(cbind(brain, longevity, ci)) ##binding new data frame with confidence interval 

names(info) <- c("x", "y", "CIfit", "CIlwr", "CIupr") #naming the columns

pi <- predict(model, newdata = data.frame(brain), interval = "prediction", level = 0.90)  ##predict prediction interval 

info_ci_pi <- cbind(info, pi) ##binding updated data frame

names(info_ci_pi) <- c("x", "y", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", "PIupr") ##updated names of columns

plot1b <- ggplot(data = info_ci_pi, aes(brain, longevity, label = rownames(info_ci_pi)))  ##drawing original plot with labels
plot1b <- plot1b + geom_point(alpha = 1/2) + geom_line(aes(x = x, y = CIfit, colour = "fit")) + geom_line(aes(x = x, y = CIlwr, colour = "CI")) + geom_line(aes(x = x, y = CIupr, colour = "CI")) + geom_line(aes(x = x, y = PIlwr, colour = "PI")) + geom_line(aes(x = x, y = PIupr, colour = "PI")) + scale_color_manual(values = c("fit" = "black","CI" = "#F96900","PI" = "#00A5E0")) + theme(legend.position = "bottom", legend.title = element_blank()) ##drawing points and lines

plot1b ##print graph 

```
because by beta1 value does not equal zero, we 

```


Part 3: Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.


Part 4: Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
log.pt.est <- predict(logm, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", 
    level = 0.90)  # for a single value
log.pt.est
```



Looking at your two models, which do you think is better? Why?

